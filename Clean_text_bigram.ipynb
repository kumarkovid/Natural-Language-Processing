{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized clean words: \n",
      " ['there', 'was', 'nothing', 'so', 'very', 'remarkable', 'in', 'that', 'nor', 'did', 'alice', 'think', 'it', 'so', 'very', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'rabbit', 'say', 'to', 'itself', 'oh', 'dear', 'oh', 'dear', 'i', 'shall', 'be', 'late', 'when', 'she', 'thought', 'it', 'over', 'afterwards', 'it', 'occurred', 'to', 'her', 'that', 'she', 'ought', 'to', 'have', 'wondered', 'at', 'this', 'but', 'at', 'the', 'time', 'it', 'all', 'seemed', 'quite', 'natural', 'but', 'when', 'the', 'rabbit', 'actually', 'took', 'a', 'watch', 'out', 'of', 'its', 'waistcoat-pocket', 'and', 'looked', 'at', 'it', 'and', 'then', 'hurried', 'on', 'alice', 'started', 'to', 'her', 'feet', 'for', 'it', 'flashed', 'across', 'her', 'mind', 'that', 'she', 'had', 'never', 'before', 'seen', 'a', 'rabbit', 'with', 'either', 'a', 'waistcoat-pocket', 'or', 'a', 'watch', 'to', 'take', 'out', 'of', 'it', 'and', 'burning', 'with', 'curiosity', 'she', 'ran', 'across', 'the', 'field', 'after', 'it', 'and', 'fortunately', 'was', 'just', 'in', 'time', 'to', 'see', 'it', 'pop', 'down', 'a', 'large', 'rabbit-hole', 'under', 'the', 'hedge']\n",
      "\n",
      "Unique word count: \n",
      " {'there': 1, 'was': 2, 'nothing': 1, 'so': 2, 'very': 2, 'remarkable': 1, 'in': 2, 'that': 3, 'nor': 1, 'did': 1, 'alice': 2, 'think': 1, 'it': 9, 'much': 1, 'out': 3, 'of': 3, 'the': 6, 'way': 1, 'to': 7, 'hear': 1, 'rabbit': 3, 'say': 1, 'itself': 1, 'oh': 2, 'dear': 2, 'i': 1, 'shall': 1, 'be': 1, 'late': 1, 'when': 2, 'she': 4, 'thought': 1, 'over': 1, 'afterwards': 1, 'occurred': 1, 'her': 3, 'ought': 1, 'have': 1, 'wondered': 1, 'at': 3, 'this': 1, 'but': 2, 'time': 2, 'all': 1, 'seemed': 1, 'quite': 1, 'natural': 1, 'actually': 1, 'took': 1, 'a': 5, 'watch': 2, 'its': 1, 'waistcoat-pocket': 2, 'and': 4, 'looked': 1, 'then': 1, 'hurried': 1, 'on': 1, 'started': 1, 'feet': 1, 'for': 1, 'flashed': 1, 'across': 2, 'mind': 1, 'had': 1, 'never': 1, 'before': 1, 'seen': 1, 'with': 2, 'either': 1, 'or': 1, 'take': 1, 'burning': 1, 'curiosity': 1, 'ran': 1, 'field': 1, 'after': 1, 'fortunately': 1, 'just': 1, 'see': 1, 'pop': 1, 'down': 1, 'large': 1, 'rabbit-hole': 1, 'under': 1, 'hedge': 1}\n",
      "\n",
      "Top words in text: \n",
      " [('it', 9), ('to', 7), ('the', 6), ('a', 5), ('she', 4), ('and', 4), ('that', 3), ('rabbit', 3), ('out', 3), ('of', 3)]\n",
      "\n",
      "Top bigrams in text: \n",
      " [((('out', 'of'),), 3), ((('it', 'and'),), 3), ((('so', 'very'),), 2), ((('the', 'rabbit'),), 2), ((('oh', 'dear'),), 2), ((('to', 'her'),), 2), ((('that', 'she'),), 2), ((('a', 'watch'),), 2), ((('there', 'was'),), 1), ((('was', 'nothing'),), 1)]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens=[]\n",
    "    clean_text=text.replace('-\\n','-')\n",
    "    clean_text=clean_text.lower()\n",
    "    clean_text=clean_text.strip()\n",
    "    clean_text=clean_text.split()\n",
    "    for element in clean_text:\n",
    "        element=element.replace('\\n',' ')\n",
    "        element=element.strip()\n",
    "        a=element.strip(string.punctuation)\n",
    "        tokens.append(a)\n",
    "    while '' in tokens:\n",
    "        tokens.remove('')\n",
    "    \n",
    "    return tokens\n",
    "    \n",
    "class Text_Analyzer(object):\n",
    "    def __init__(self,text,token_count={}):              #Initializing \n",
    "        self.text=text\n",
    "        self.token_count=token_count\n",
    "           \n",
    "    def analyze(self):\n",
    "        token_count={}\n",
    "        tokenized=tokenize(analyzer.text)                #Used analyser object to access the text\n",
    "        for iterate in tokenized:\n",
    "            if iterate not in token_count:\n",
    "                token_count[iterate] = 1 \n",
    "            else:\n",
    "                token_count[iterate] += 1\n",
    "        return token_count\n",
    "        \n",
    "    def topN(self,N):\n",
    "        unsorted_dict=Text_Analyzer.analyze(self)        #Using class to access its function(token_count)\n",
    "        sorted_list=[]\n",
    "        for keys, values in sorted(unsorted_dict.items(), key=lambda k: (k[1], k[0]),reverse=True):\n",
    "            sorted_list.append((keys, values))    \n",
    "        return sorted_list[0:N]       \n",
    "\n",
    "def bigram(doc,N):\n",
    "    clean_tokens=tokenize(doc)\n",
    "    result = []\n",
    "    for i in range(len(clean_tokens)-1):                                        #for every index(i) combine i and i+1\n",
    "        result.append((clean_tokens[i], clean_tokens[i+1]))   \n",
    "    sorted_bigram_count=Counter(zip(result))\n",
    "    result = sorted_bigram_count.most_common(N)\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":  \n",
    "    \n",
    "    # Test Question 1\n",
    "    sample_text=''' There was nothing so VERY remarkable in that; nor did Alice\n",
    "think it so VERY much out of the way to hear the Rabbit say to\n",
    "itself, `Oh dear!  Oh dear!  I shall be late!'  (when she thought\n",
    "it over afterwards, it occurred to her that she ought to have\n",
    "wondered at this, but at the time it all seemed quite natural);\n",
    "but when the Rabbit actually TOOK A WATCH OUT OF ITS WAISTCOAT-\n",
    "POCKET, and looked at it, and then hurried on, Alice started to\n",
    "her feet, for it flashed across her mind that she had never\n",
    "before seen a rabbit with either a waistcoat-pocket, or a watch to\n",
    "take out of it, and burning with curiosity, she ran across the\n",
    "field after it, and fortunately was just in time to see it pop\n",
    "down a large rabbit-hole under the hedge.\n",
    "'''       \n",
    "\n",
    "print(\"\\nTokenized clean words: \\n\",tokenize(sample_text))\n",
    "analyzer=Text_Analyzer(sample_text)\n",
    "print(\"\\nUnique word count: \\n\",analyzer.analyze())\n",
    "\n",
    "print(\"\\nTop words in text: \\n\",analyzer.topN(10)) \n",
    "#Analysis: The words which occur the most generally are the stop words. They do not provide any information of the type of \n",
    "#document/text we are processing. Therefore, there is no use of such words in the text processing nd wnt be help to help \n",
    "#us in any kind to sentiment analysis or judging the nature of the document we are processing.   \n",
    "\n",
    "top_bigrams=bigram(text,10)\n",
    "print(\"\\nTop bigrams in text: \\n\",top_bigrams)\n",
    "#Analysis: The bigrams showcases some good phrases like ('the', 'rabbit'), which helps in determining \n",
    "#about the context of the text.Therefore it happens to deliver more meaning than the most common individual words. \n",
    "#Therefore, it is a better way to study text documents than the most common individual words method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
